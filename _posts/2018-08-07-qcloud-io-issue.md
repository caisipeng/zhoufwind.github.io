---
layout: post
title: "腾讯云物理硬盘固件版本bug事故回顾"
subtitle: "管好自己的命根子，做好高可用，别让别人毁了它！"
date: 2018-08-07 09:19PM
catalog: true
tags:
    - 运维
    - 云计算
    - 高可用
---

本周运维圈的热点非腾讯云莫属，借此话题来聊聊自己的一些看法。

### 背景

事件具体发生经过相信大家应该都已经很清楚了，大概经过就是有个叫“前沿数控”的互联网创业公司，他们的产线服务器部署在腾讯云上，腾讯云在7月20日有次云存储故障，并且最终确认已无法恢复数据，导致前沿数控产线数据丢失，前沿数据号称自己因丢失数据所造成的损失价值千万级别，但腾讯云表示只会提供13万余元的现金或云资源作为补偿，双方因此产生分歧的故事。

还不清楚的同学可以搜关键字：“前沿数控”，或是腾讯云的[公告][1]：《关于用户“前沿数控”数据完整性受损及腾讯云补偿措施的说明》，以了解最新动态。

我们只对问题本身进行讨论，不过多讨论谁对谁错，毕竟事情已经发生了，对于我们来说，如何从中吸取教训，保证自己不会重蹈覆辙才是最重要的，上面两家最终谁输谁赢，与我们关系并不大。

### 问题

在我看来，有以下三件事，绝对是称得上运维的三大噩梦了，而且严重等级依次递增：

1. 重启后服务没恢复  
   重启服务、应用或硬件后，没能按照预期恢复原先状态，并且无法在短时间内快速修复，造成线上不可用。  
   但只要线上部署了高可用服务，无论是有状态（MySQL/Redis/ES等）、无状态服务（Tomcat/IIS/Django等），或是硬件设备（F5/交换机/防火墙等），都是可以相对较快的恢复线上服务的。

2. 没有HA（高可用架构）  
   如果在部署业务时，由于某些原因，没有部署高可用架构，业务存在单点，单节点服务挂掉后，将会直接造成线上长时间不可用。  
   但只要服务配置有备份，重新部署一套业务也能恢复线上服务，只是耗费的时间可能相对较长一些。

3. 数据丢了而且没备份  
   单点无人维护的程序意外被删，服务器根目录意外被删、数据库意外被删、服务器意外宕机无法启动、服务器硬盘意外损坏切无法通过RAID恢复等等，这些情形极有可能造成此类情况。  
   当出现这种情况时，一般情况下真的很难，或者需要花费很长很长时间才能恢复正常业务了。

这三招，招招致命，时刻威胁着线上服务稳定性。并不是我在危言耸听，列出的每一项都有活生生的案例，几乎每项都能找到对应具体的事故，而且不乏大厂传出来的。

腾讯云这次非常不幸，踩到了第三个坑，最终导致业务无法恢复。

### 故障原因

在写这篇文章的时候，腾讯云的[故障报告][2]：《关于客户“前沿数控”数据完整性受损的技术复盘》正好发出来了，看了下故障原因，大致是因为某机房云存储空间不足，需要扩容，扩容操作行为不规范导致的。

腾讯云存储扩容操作分两步：
- 将包含前沿数控所在云服务器的磁盘A，在线迁移至大容量磁盘B（磁盘A、B仅用作举例，实际另有名称）  
  按腾讯云的规定，正常情况下，需要10小时才能完成数据迁移，但有个安全开关，只要把它关了，就能提速到5小时内完成，但是会牺牲数据完整性。 （此处10小时、5小时仅用作举例，具体时间不详）  
  猜想迁移任务应该十分紧迫，运维同学不得不关闭了这个安全开关，加速完成迁移操作，但没发现实际在传输过程中，数据已出现损坏。

- 迁移完成后，删除原磁盘A上的老数据，以释放空间  
  按腾讯云的规定，正常情况下，老数据需要保留24小时后，才能删除，但运维同学同样为了加速释放空间，在发现磁盘B的数据异常之前就已经把数据删除了，造成了不可逆的问题。

由此看来，按照腾讯云的故障报告来看，此次故障完全是由于人为操作违规所致，如果按照正常流程闭环操作的话，还是能够达到所谓的“9个9”的可用性。

但是，数据丢了就是丢了，对客户而言，不管是不是千万元级别损失，肯定是被坑了一把，对吧？

### 类似经历

我司也遇到过类似问题，虽不是云服务器的问题，但也是云服务一种——CDN。

我还清晰记得是在16年的平安夜那晚，我和当时的女朋友（现任老婆）在云南丽江旅游，晚上11点左右时候，微信群突然爆炸，有人更是直接把CEO的反馈内容转了过来，用户访问我司大部分域名异常返回“502 Bad Gateway”，业务同学确认源站正常，定位是CDN问题。

之前是特别信任这家CDN供应商，基本整站都配在了他家。恰巧当晚的故障又是前所未有的严重，不是部分节点问题，不是部分域名问题，是所有节点所有域名都有问题，线上瞬间基本没有了可用性，还是在晚高峰时间。当时我都紧张的和我老婆说，可能要飞回上海了。

我司在当时并没有立即切换至备线CDN的能力，无奈只能将所有域名切至回源，访问质量有所降低，但至少可以保证一部分用户可用。

虽说当时确实也有备用CDN线路，但是是冷备，已经很久没用了，并且大部分域名配置都是老的，切换至备线后也存在这样那样的问题，无法正常使用。

最后直到第二天白天，原CDN供应商逐渐修复问题后，这边再逐渐切回原CDN，完全恢复线上服务。

这次故障是我从事运维工作以来，遇到的最大一次故障，从这次故障中我吸取了一个教训：

> 不要完全相信任何一家CDN供应商，甚至包括自己的服务。不要太过自信，管好自己的命根子，做好高可用，别让别人毁了它！

从那天开始，我都一直都在往这个目标前进，而且会坚持这个信念走的更远。

我司现在已经能够实现，线上同时部署主备两至三家不同CDN供应商，但凡某家出现大规模且无法及时恢复的故障，10分钟之内完成探测、线路切换、故障解除的能力。

### 后记

数据无价，前沿数控索赔千万，无疑就是想表达自己的愤怒、痛苦和委屈，为自己的心血及汗水讨回个公道，能不能真赔到千万前沿数控自己也不一定有把握。

不过讲道理，前沿数控既然能SSH登录云服务器，猜想肯定也有开发或者运维的同学在运营着自己的服务，如果这些同学能多考虑一点高可用或是灾备方面的问题，不要把自己的命根子完全托付在别人手上，现在也不会如此被动了。

[1]: https://mp.weixin.qq.com/s/9bP6abnvOpufN3fomB4weQ
[2]: https://mp.weixin.qq.com/s/8JSPY6vHPhg8pX0JwjqttQ